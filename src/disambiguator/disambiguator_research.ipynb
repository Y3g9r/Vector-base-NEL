{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10309577",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\y3g9r\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import  Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizerFast, BertModel\n",
    "import torch.optim as optim\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import datetime as dt\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef96a95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bb0542d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisambiguationDataset(Dataset):\n",
    "    def __init__(self, samples,labels):\n",
    "        self.samples = samples\n",
    "        self.labels = labels\n",
    "        self.len = len(self.samples)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        items = {\"text_input_ids\": torch.tensor(self.samples[index][0]),\n",
    "                 \"text_input_mask\": torch.tensor(self.samples[index][1]),\n",
    "                 \"text_segment_ids\": torch.tensor(self.samples[index][2]),\n",
    "                 \"text_offset_mapping\": torch.tensor(self.samples[index][3]),\n",
    "                 \"text_pos\": torch.tensor(self.samples[index][4]),\n",
    "                 \"def_input_ids\": torch.tensor(self.samples[index][5]),\n",
    "                 \"def_input_mask\": torch.tensor(self.samples[index][6]),\n",
    "                 \"def_segment_ids\": torch.tensor(self.samples[index][7]),\n",
    "                 \"label\": torch.tensor(self.labels[index])}\n",
    "        return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "269a9166",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NerualNet(nn.Module):\n",
    "    def __init__(self, hidden_size=768, max_seq_len=388, device = 'cpu'):\n",
    "        self.device = device\n",
    "        super(NerualNet, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('sberbank-ai/sbert_large_mt_nlu_ru')\n",
    "        for layer in self.bert.encoder.layer[:11]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.text_pooling = nn.MaxPool1d(kernel_size=max_seq_len, stride=1)\n",
    "        self.def_pooling = nn.MaxPool1d(kernel_size=max_seq_len, stride=1)\n",
    "\n",
    "        self.cos = torch.nn.CosineSimilarity()\n",
    "\n",
    "        self.sigm = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, text_input_ids, text_input_mask, text_segment_ids, text_offset_mapping,\n",
    "                text_pos, def_input_ids, def_input_mask, def_segment_ids):\n",
    "\n",
    "        embd_batch = torch.tensor([[[], []]]).to(self.device)\n",
    "        for i in range(len(text_input_ids)):\n",
    "            # получаем эмбединги ключевого слова из примера употребления\n",
    "            examples_token_key_word_position = self.token_detection(text_offset_mapping[i][0], text_pos[i])\n",
    "            example_token_vec = self.get_vector(text_input_ids[i], text_segment_ids[i], text_input_mask[i])\n",
    "            example_embeddings = self.vector_recognition(example_token_vec, examples_token_key_word_position)\n",
    "\n",
    "            # получаем эмбединг определения\n",
    "            def_embedding = self.get_defenition_embedding(def_input_ids[i], def_segment_ids[i],\n",
    "                                                          def_input_mask[i])\n",
    "            # объединяем два вектора в 1 и добавляем в общий массив (получаем тензор 2x768)\n",
    "            embd_sample = torch.stack((example_embeddings, def_embedding))\n",
    "            if not first_pass:\n",
    "                embd_batch = torch.cat((embd_batch, embd_sample.unsqueeze(0)), -1)\n",
    "                first_pass = True\n",
    "            else:\n",
    "                embd_batch = torch.cat((embd_batch, embd_sample.unsqueeze(0)), 0)\n",
    "\n",
    "        text_emb = embd_batch[:, 0, :]\n",
    "        def_emb = embd_batch[:, 1, :]\n",
    "\n",
    "        text_pool = self.text_pooling(text_emb)\n",
    "        def_pool = self.def_pooling(def_emb)\n",
    "\n",
    "        cos_comp = self.cos(text_pool, def_pool)\n",
    "\n",
    "        y = self.sigm(cos_comp)\n",
    "\n",
    "        return y\n",
    "\n",
    "    def get_defenition_embedding(self, def_input_ids, def_segment_ids, def_input_mask):\n",
    "        \"\"\"\n",
    "        Функция получения вектора дефенишина сущности\n",
    "        :param def_input_ids:\n",
    "        :param def_segment_ids:\n",
    "        :param def_input_mask:\n",
    "        :return: bert pooler output vector\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            output = self.bert(input_ids=def_input_ids, token_type_ids=def_segment_ids,\n",
    "                                attention_mask=def_input_mask)\n",
    "        hidden_states = output[1]\n",
    "        return hidden_states\n",
    "\n",
    "    def token_detection(self, token_map, position):\n",
    "        \"\"\"\n",
    "        Функция определения ключевого слова\n",
    "        :param token_map: list of tuples of begin and end of every token\n",
    "        :param position:  list of type: [int,int]\n",
    "        :return: list of key word tokens position\n",
    "        \"\"\"\n",
    "        # из за того что в начале стоит CLS позиции начала и конца ключевого слова сдвигаются на 5\n",
    "        begin_postion = position[0]  # + 5\n",
    "        end_position = position[1]  # + 5\n",
    "\n",
    "        position_of_key_tokens = []\n",
    "        for token_tuple in range(1, len(token_map) - 1):\n",
    "            # Если ключевое слово представляется одним токеном\n",
    "            if token_map[token_tuple][0] == begin_postion and token_map[token_tuple][1] == end_position:\n",
    "                position_of_key_tokens.append(token_tuple)\n",
    "                break\n",
    "\n",
    "            # Если ключевое слово представляется несколькими токенами\n",
    "            if token_map[token_tuple][0] >= begin_postion and token_map[token_tuple][1] != end_position:\n",
    "                position_of_key_tokens.append(token_tuple)\n",
    "            if token_map[token_tuple][0] != begin_postion and token_map[token_tuple][1] == end_position:\n",
    "                position_of_key_tokens.append(token_tuple)\n",
    "                break\n",
    "\n",
    "        return position_of_key_tokens\n",
    "\n",
    "    def get_vector(self, input_ids_samp, token_type_ids_samp, attention_mask_samp):\n",
    "        \"\"\"\n",
    "        Функция получения вектора ключевого слова\n",
    "        :param input_ids_samp:\n",
    "        :param token_type_ids_samp:\n",
    "        :param attention_mask_samp:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids=input_ids_samp, token_type_ids=token_type_ids_samp,\n",
    "                                 attention_mask=attention_mask_samp)\n",
    "            hidden_states = outputs[2]\n",
    "\n",
    "        token_dim = torch.stack(hidden_states, dim=0)\n",
    "        token_dim = torch.squeeze(token_dim, dim=1)\n",
    "        token_dim = token_dim.permute(1, 0, 2)\n",
    "        token_vecs_cat = []\n",
    "        for token in token_dim:\n",
    "            cat_vec = torch.sum(token[-4:], dim=0)\n",
    "            token_vecs_cat.append(cat_vec)\n",
    "\n",
    "        return token_vecs_cat\n",
    "\n",
    "    def vector_recognition(self, tokens_embeddings_ex, tokens_key_word_position_ex):\n",
    "        \"\"\"\n",
    "        Функция подготовки вектора в зависимости от количества токенов,которым представляется ключевое слово\n",
    "        :param tokens_embeddings_ex:\n",
    "        :param tokens_key_word_position_ex:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if len(tokens_key_word_position_ex) > 1:\n",
    "            embeddings_data = torch.tensor(\n",
    "                self.__get_avarage_embedding(tokens_embeddings_ex, tokens_key_word_position_ex))\n",
    "        else:\n",
    "            # print(tokens_embeddings_ex)\n",
    "            # print(tokens_key_word_position_ex)\n",
    "            embeddings_data = torch.tensor(tokens_embeddings_ex[tokens_key_word_position_ex[0]])\n",
    "        return embeddings_data\n",
    "\n",
    "    def __get_avarage_embedding(self, embeddings_list, positions_list):\n",
    "        \"\"\"\n",
    "        Функция получения среднего вектора (применяется в случае если ключевое слово состоит из нескольких токенов)\n",
    "        :param embeddings_list:\n",
    "        :param positions_list:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        avg_tensor = torch.stack((embeddings_list[positions_list[0]],))\n",
    "        for i in range(1, len(positions_list)):\n",
    "            avg_tensor = torch.cat((avg_tensor, embeddings_list[positions_list[i]].unsqueeze(0)))\n",
    "\n",
    "        average_embedding = torch.mean(avg_tensor, 0)\n",
    "        return average_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c66ce6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, num_epochs=None, batch_size=None,\n",
    "                 max_batches_per_epoch=None, early_stopping=10,\n",
    "                 loss_fn=None, optimizer=None, model=None,\n",
    "                 scheduler=None, device='cpu'):\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.max_batches_per_epoch = max_batches_per_epoch\n",
    "        self.early_stopping = early_stopping\n",
    "        self.loss_fn = loss_fn\n",
    "        self.device = device\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.start_model = model\n",
    "        self.best_model = model\n",
    "\n",
    "        self.train_loss = []\n",
    "        self.valid_loss = []\n",
    "\n",
    "    def predict(self, input_ids, input_mask, segment_ids):\n",
    "        return self.best_model(input_ids, input_mask, segment_ids)\n",
    "\n",
    "    def save_model(self, path: str):\n",
    "        try:\n",
    "            torch.save(self.best_model, path)\n",
    "        except Exception as e:\n",
    "            print(f\"Не удалось сохранить модель. Ошибка {e}\")\n",
    "            exit(1)\n",
    "\n",
    "        return True\n",
    "\n",
    "    def load_model(self, path: str):\n",
    "        try:\n",
    "            self.best_model.load_state_dict(torch.load(path))\n",
    "        except Exception as e:\n",
    "            print(f\"Не удалось загрузить модель. Ошибка {e}\")\n",
    "            exit(1)\n",
    "\n",
    "        return True\n",
    "\n",
    "    def fit(self, train_dataset, valid_dataset):\n",
    "\n",
    "        device = torch.device(self.device)\n",
    "        print(device)\n",
    "        NerualNet = self.start_model\n",
    "        NerualNet.to(device)\n",
    "\n",
    "        NerualNet.train()\n",
    "\n",
    "        self.optimizer = optim.Adam(NerualNet.parameters(), lr=0.0001)\n",
    "\n",
    "        train_loader = DataLoader(dataset=train_dataset, batch_size=self.batch_size,\n",
    "                                  shuffle=False, drop_last=True)\n",
    "        valid_loader = DataLoader(dataset=valid_dataset, batch_size=self.batch_size,\n",
    "                                  shuffle=False, drop_last=True)\n",
    "\n",
    "        best_val_loss = float('inf')  # Лучшее значение функции потерь на валидационной выборке\n",
    "\n",
    "        best_ep = 0  # Эпоха, на которой достигалось лучшее значение функции потерь на валидационной выборке\n",
    "\n",
    "        for epoch in range(self.num_epochs):\n",
    "            start = dt.datetime.now()\n",
    "            mean_loss = 0\n",
    "            batch_n = 0\n",
    "            for batch in train_loader:\n",
    "                y_truth = batch[\"label\"].float().to(device)\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                input_mask = batch[\"input_mask\"].to(device)\n",
    "                segment_ids = batch[\"segment_ids\"].to(device)\n",
    "\n",
    "                y_pred = NerualNet(input_ids, input_mask, segment_ids).float()\n",
    "                loss = self.loss_fn(y_pred, y_truth)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                del batch\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "\n",
    "                mean_loss += float(loss)\n",
    "                batch_n += 1\n",
    "\n",
    "            mean_loss /= batch_n\n",
    "            self.train_loss.append(mean_loss)\n",
    "            print(f'Эпоха: {epoch + 1}\\n Train loss: {mean_loss}\\n {dt.datetime.now() - start} сек.\\n')\n",
    "\n",
    "            NerualNet.eval()\n",
    "            mean_loss = 0\n",
    "            batch_n = 0\n",
    "            with torch.no_grad():\n",
    "                for batch in valid_loader:\n",
    "                    if self.max_batches_per_epoch is not None:\n",
    "                        if batch_n >= self.max_batches_per_epoch:\n",
    "                            break\n",
    "\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                input_mask = batch[\"input_mask\"].to(device)\n",
    "                segment_ids = batch[\"segment_ids\"].to(device)\n",
    "                target = batch[\"label\"].float().to(device)\n",
    "\n",
    "                predicted_values = NerualNet(input_ids, input_mask, segment_ids).float()\n",
    "                loss = self.loss_fn(predicted_values, target)\n",
    "\n",
    "                del batch\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "\n",
    "                mean_loss += float(loss)\n",
    "                batch_n += 1\n",
    "\n",
    "            mean_loss /= batch_n\n",
    "            self.valid_loss.append(mean_loss)\n",
    "            print(f'Loss_val: {mean_loss}')\n",
    "\n",
    "            if mean_loss < best_val_loss:\n",
    "                self.best_model = NerualNet\n",
    "                best_val_loss = mean_loss\n",
    "                best_ep = epoch\n",
    "            elif epoch - best_ep > self.early_stopping:\n",
    "                print(f'{self.early_stopping} без улучшений. Прекращаем обучение...')\n",
    "                break\n",
    "            if self.scheduler is not None:\n",
    "                scheduler.step()\n",
    "            print()\n",
    "\n",
    "        print(\"-=-=-=-=-=-=-=-=-=-= Evaluation of the best model =-=-=-=-=-=-=-=-=-=-\")\n",
    "        plt.plot(range(len(self.train_loss)), self.train_loss, color='green', label='train', linestyle='solid')\n",
    "        plt.plot(range(len(self.valid_loss)), self.valid_loss, color='red', label='val', linestyle='solid')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_test = [float(sample['label']) for sample in valid_dataset]\n",
    "            Y_pred = []\n",
    "            Y_pred = [self.best_model(sample['input_ids'].unsqueeze(0).to(device),\n",
    "                                      sample['input_mask'].unsqueeze(0).to(device),\n",
    "                                      sample['segment_ids'].unsqueeze(0).to(device)) for sample in valid_dataset]\n",
    "            Y_pred = [float(y > 0.5) for y in Y_pred]\n",
    "            print()\n",
    "\n",
    "            print(f\"report: \\n\", classification_report(y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6dd98c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preparation(texts, definitions, position, labels, tokenizer, max_len):\n",
    "    tokenizer = tokenizer\n",
    "    feautures_X, feautures_Y = [], []\n",
    "\n",
    "    for i, (text, definition) in enumerate(zip(texts, definitions)):\n",
    "        text = tokenizer(text, return_offsets_mapping=True,max_length=max_len,truncation=True)\n",
    "\n",
    "        text_input_ids = text[\"input_ids\"]\n",
    "        text_input_mask = text[\"attention_mask\"]\n",
    "        text_segment_ids = text[\"token_type_ids\"]\n",
    "        text_offset_mapping = text[\"offset_mapping\"]\n",
    "        text_pos = [position[i]]\n",
    "\n",
    "        definition = tokenizer(definition, return_offsets_mapping=True,max_length=max_len,truncation=True)\n",
    "\n",
    "        def_input_ids = definition[\"input_ids\"]\n",
    "        def_input_mask = definition[\"attention_mask\"]\n",
    "        def_segment_ids = definition[\"token_type_ids\"]\n",
    "\n",
    "        feautures_X.append([text_input_ids, text_input_mask, text_segment_ids, text_offset_mapping,\n",
    "                            text_pos, def_input_ids, def_input_mask, def_segment_ids])\n",
    "        feautures_Y.append(labels[i])\n",
    "\n",
    "    return feautures_X, feautures_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dd3b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../nn_data.csv')\n",
    "\n",
    "max_len_text = df.text.str.len().max()\n",
    "max_len_def = df.definition.str.len().max()\n",
    "\n",
    "max_len = max_len_def\n",
    "if max_len_text > max_len_def:\n",
    "    max_len = max_len_text\n",
    "\n",
    "data_X, data_Y = data_preparation(df.text,\n",
    "                                  df.definition,\n",
    "                                  df.position,\n",
    "                                  df.label,\n",
    "                                  BertTokenizerFast.from_pretrained('sberbank-ai/sbert_large_mt_nlu_ru',\n",
    "                                                                do_lower_case=True),\n",
    "                                  max_len)\n",
    "\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(data_X, data_Y, test_size = 0.2, random_state=42)\n",
    "\n",
    "train_dataset = DisambiguationDataset(train_X, train_Y)\n",
    "test_dataset = DisambiguationDataset(test_X, test_Y)\n",
    "\n",
    "trainer = Trainer(num_epochs=40,\n",
    "                  batch_size=8,\n",
    "                  loss_fn=nn.BCELoss(),\n",
    "                  model=NerualNet(max_seq_len=max_len, device='cuda:0'),\n",
    "                  device='cuda:0')\n",
    "\n",
    "trainer.fit(train_dataset=train_dataset, valid_dataset=test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
