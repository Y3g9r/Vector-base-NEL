{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10309577",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\y3g9r\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import  Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizerFast, BertModel\n",
    "import torch.optim as optim\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import ast\n",
    "import datetime as dt\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760be49e",
   "metadata": {},
   "source": [
    "# Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bb0542d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisambiguationDataset(Dataset):\n",
    "    def __init__(self, samples,labels):\n",
    "        self.samples = samples\n",
    "        self.labels = labels\n",
    "        self.len = len(self.samples)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        items = {\"text_input_ids\": torch.tensor(self.samples[index][0]),\n",
    "                 \"text_input_mask\": torch.tensor(self.samples[index][1]),\n",
    "                 \"text_segment_ids\": torch.tensor(self.samples[index][2]),\n",
    "                 \"text_offset_mapping\": torch.tensor(self.samples[index][3]),\n",
    "                 \"text_pos\": torch.tensor(self.samples[index][4]),\n",
    "                 \"def_input_ids\": torch.tensor(self.samples[index][5]),\n",
    "                 \"def_input_mask\": torch.tensor(self.samples[index][6]),\n",
    "                 \"def_segment_ids\": torch.tensor(self.samples[index][7]),\n",
    "                 \"label\": torch.tensor(self.labels[index])}\n",
    "        return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "269a9166",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NerualNet(nn.Module):\n",
    "    def __init__(self, hidden_size=768, max_seq_len=388, device='cpu'):\n",
    "        self.device = device\n",
    "        super(NerualNet, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('sberbank-ai/sbert_large_mt_nlu_ru', output_hidden_states=True,\n",
    "                                              return_dict=False)\n",
    "        for layer in self.bert.encoder.layer[:20]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.text_linear_1 = torch.nn.Linear(1024, max_seq_len)\n",
    "        self.def_linear_1 = torch.nn.Linear(1024, max_seq_len)\n",
    "        \n",
    "        self.Dropout_text = torch.nn.Dropout(0.5)\n",
    "        self.Dropout_def = torch.nn.Dropout(0.5)\n",
    "        \n",
    "        self.sigm_linear_1 = torch.nn.Linear(max_seq_len, 1)\n",
    "        self.Dropout_sigm = torch.nn.Dropout(0.5)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, text_input_ids, text_input_mask, text_segment_ids, text_offset_mapping,\n",
    "                text_pos, def_input_ids, def_input_mask, def_segment_ids):\n",
    "\n",
    "        embd_batch = torch.tensor([[[], []]]).to(self.device)\n",
    "        first_pass = False\n",
    "        for i in range(len(text_input_ids)):\n",
    "            # получаем эмбединги ключевого слова из примера употребления\n",
    "            examples_token_key_word_position = self.token_detection(text_offset_mapping[i], text_pos[i][0])\n",
    "            example_token_vec = self.get_vector(text_input_ids[i], text_segment_ids[i], text_input_mask[i])\n",
    "            example_embeddings = self.vector_recognition(example_token_vec, examples_token_key_word_position)\n",
    "\n",
    "            # получаем эмбединг определения\n",
    "            def_embedding = self.get_defenition_embedding(def_input_ids[i], def_segment_ids[i],\n",
    "                                                          def_input_mask[i]).squeeze(0)\n",
    "            # объединяем два вектора в 1 и добавляем в общий массив (получаем тензор 2x768)\n",
    "            embd_sample = torch.stack((example_embeddings, def_embedding)).to(self.device)\n",
    "            if not first_pass:\n",
    "                embd_batch = torch.cat((embd_batch, embd_sample.unsqueeze(0)), -1)\n",
    "                first_pass = True\n",
    "            else:\n",
    "                embd_batch = torch.cat((embd_batch, embd_sample.unsqueeze(0)), 0)\n",
    "\n",
    "                \n",
    "        text_emb = embd_batch[:, 0, :]\n",
    "        def_emb = embd_batch[:, 1, :]\n",
    "\n",
    "        ex_emb = self.Dropout_text(self.text_linear_1(text_emb))\n",
    "        def_emb = self.Dropout_def(self.def_linear_1(def_emb))\n",
    "\n",
    "        dist = torch.abs(ex_emb-def_emb)\n",
    "        py = self.Dropout_sigm(self.sigm_linear_1(dist))\n",
    "\n",
    "        y = self.sigmoid(py).permute(1,0).squeeze(0)\n",
    "\n",
    "        return y\n",
    "\n",
    "    def get_defenition_embedding(self, def_input_ids, def_segment_ids, def_input_mask):\n",
    "        \"\"\"\n",
    "        Функция получения вектора дефенишина сущности\n",
    "        :param def_input_ids:\n",
    "        :param def_segment_ids:\n",
    "        :param def_input_mask:\n",
    "        :return: bert pooler output vector\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            output = self.bert(input_ids=def_input_ids.unsqueeze(0), token_type_ids=def_segment_ids.unsqueeze(0),\n",
    "                               attention_mask=def_input_mask.unsqueeze(0))\n",
    "        hidden_states = output[1]\n",
    "        return hidden_states\n",
    "\n",
    "    def token_detection(self, token_map, position):\n",
    "        \"\"\"\n",
    "        Функция определения ключевого слова\n",
    "        :param token_map: list of tuples of begin and end of every token\n",
    "        :param position:  list of type: [int,int]\n",
    "        :return: list of key word tokens position\n",
    "        \"\"\"\n",
    "        # из за того что в начале стоит CLS позиции начала и конца ключевого слова сдвигаются на 5\n",
    "        begin_postion = position[0]  # + 5\n",
    "        end_position = position[1]  # + 5\n",
    "\n",
    "        position_of_key_tokens = []\n",
    "        for token_tuple in range(1, len(token_map) - 1):\n",
    "            # Если ключевое слово представляется одним токеном\n",
    "            if token_map[token_tuple][0] == begin_postion and token_map[token_tuple][1] == end_position:\n",
    "                position_of_key_tokens.append(token_tuple)\n",
    "                break\n",
    "\n",
    "            # Если ключевое слово представляется несколькими токенами\n",
    "            if token_map[token_tuple][0] >= begin_postion and token_map[token_tuple][1] != end_position:\n",
    "                position_of_key_tokens.append(token_tuple)\n",
    "            if token_map[token_tuple][0] != begin_postion and token_map[token_tuple][1] == end_position:\n",
    "                position_of_key_tokens.append(token_tuple)\n",
    "                break\n",
    "\n",
    "        return position_of_key_tokens\n",
    "\n",
    "    def get_vector(self, input_ids_samp, token_type_ids_samp, attention_mask_samp):\n",
    "        \"\"\"\n",
    "        Функция получения вектора ключевого слова\n",
    "        :param input_ids_samp:\n",
    "        :param token_type_ids_samp:\n",
    "        :param attention_mask_samp:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            outputs = self.bert(input_ids=input_ids_samp.unsqueeze(0), token_type_ids=token_type_ids_samp.unsqueeze(0),\n",
    "                                attention_mask=attention_mask_samp.unsqueeze(0))\n",
    "        hidden_states = outputs[2]\n",
    "\n",
    "        # из [# layers, # batches, # tokens, # features]\n",
    "        # в [# tokens, # layers, # features]\n",
    "        token_dim = torch.stack(hidden_states, dim=0)\n",
    "        token_dim = torch.squeeze(token_dim, dim=1)\n",
    "        token_dim = token_dim.permute(1, 0, 2)\n",
    "        token_vecs_cat = []\n",
    "        for token in token_dim:\n",
    "            cat_vec = torch.sum(token[-4:], dim=0)\n",
    "            token_vecs_cat.append(cat_vec)\n",
    "        return token_vecs_cat\n",
    "\n",
    "\n",
    "    def vector_recognition(self, tokens_embeddings_ex, tokens_key_word_position_ex):\n",
    "        \"\"\"\n",
    "        Функция подготовки вектора в зависимости от количества токенов,которым представляется ключевое слово\n",
    "        :param tokens_embeddings_ex:\n",
    "        :param tokens_key_word_position_ex:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if len(tokens_key_word_position_ex) > 1:\n",
    "            embeddings_data = torch.tensor(\n",
    "                self.__get_avarage_embedding(tokens_embeddings_ex, tokens_key_word_position_ex))\n",
    "        else:\n",
    "            embeddings_data = torch.tensor(tokens_embeddings_ex[tokens_key_word_position_ex[0]])\n",
    "        return embeddings_data\n",
    "\n",
    "    def __get_avarage_embedding(self, embeddings_list, positions_list):\n",
    "        \"\"\"\n",
    "        Функция получения среднего вектора (применяется в случае если ключевое слово состоит из нескольких токенов)\n",
    "        :param embeddings_list:\n",
    "        :param positions_list:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        avg_tensor = torch.stack((embeddings_list[positions_list[0]],))\n",
    "        for i in range(1, len(positions_list)):\n",
    "            avg_tensor = torch.cat((avg_tensor, embeddings_list[positions_list[i]].unsqueeze(0)))\n",
    "\n",
    "        average_embedding = torch.mean(avg_tensor, 0)\n",
    "        return average_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c66ce6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, num_epochs=None, batch_size=None,\n",
    "                 max_batches_per_epoch=None, early_stopping=10,\n",
    "                 loss_fn=None, optimizer=None, model=None,\n",
    "                 scheduler=None, device='cpu'):\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.max_batches_per_epoch = max_batches_per_epoch\n",
    "        self.early_stopping = early_stopping\n",
    "        self.loss_fn = loss_fn\n",
    "        self.device = device\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.start_model = model\n",
    "        self.best_model = model\n",
    "\n",
    "        self.train_loss = []\n",
    "        self.valid_loss = []\n",
    "\n",
    "    def predict(self, input_ids, input_mask, segment_ids):\n",
    "        return self.best_model(input_ids, input_mask, segment_ids)\n",
    "\n",
    "    def save_model(self, path: str):\n",
    "        try:\n",
    "            torch.save(self.best_model, path)\n",
    "        except Exception as e:\n",
    "            print(f\"Не удалось сохранить модель. Ошибка {e}\")\n",
    "            exit(1)\n",
    "\n",
    "        return True\n",
    "\n",
    "    def load_model(self, path: str):\n",
    "        try:\n",
    "            self.best_model.load_state_dict(torch.load(path))\n",
    "        except Exception as e:\n",
    "            print(f\"Не удалось загрузить модель. Ошибка {e}\")\n",
    "            exit(1)\n",
    "\n",
    "        return True\n",
    "\n",
    "    def fit(self, train_dataset, valid_dataset):\n",
    "        device = torch.device(self.device)\n",
    "        NerualNet = self.start_model\n",
    "        NerualNet.to(device)\n",
    "\n",
    "        NerualNet.train()\n",
    "\n",
    "        self.optimizer = optim.Adam(NerualNet.parameters(), lr=0.0001)\n",
    "\n",
    "        train_loader = DataLoader(dataset=train_dataset, batch_size=self.batch_size,\n",
    "                                  shuffle=False, drop_last=True)\n",
    "        valid_loader = DataLoader(dataset=valid_dataset, batch_size=self.batch_size,\n",
    "                                  shuffle=False, drop_last=True)\n",
    "\n",
    "        best_val_loss = float('inf')  # Лучшее значение функции потерь на валидационной выборке\n",
    "\n",
    "        best_ep = 0  # Эпоха, на которой достигалось лучшее значение функции потерь на валидационной выборке\n",
    "\n",
    "        for epoch in range(self.num_epochs):\n",
    "            start = dt.datetime.now()\n",
    "            mean_loss = 0\n",
    "            batch_n = 0\n",
    "            for batch in train_loader:\n",
    "                y_truth = batch[\"label\"].float().to(device)\n",
    "                text_input_ids = batch[\"text_input_ids\"].to(device)\n",
    "                text_input_mask = batch[\"text_input_mask\"].to(device)\n",
    "                text_segment_ids = batch[\"text_segment_ids\"].to(device)\n",
    "                text_offset_mapping = batch[\"text_offset_mapping\"].to(device)\n",
    "                text_pos = batch[\"text_pos\"].to(device)\n",
    "                def_input_ids = batch[\"def_input_ids\"].to(device)\n",
    "                def_input_mask = batch[\"def_input_mask\"].to(device)\n",
    "                def_segment_ids = batch[\"def_segment_ids\"].to(device)\n",
    "                y_pred = NerualNet(text_input_ids, text_input_mask, text_segment_ids, text_offset_mapping,\n",
    "                                   text_pos, def_input_ids, def_input_mask, def_segment_ids).float()\n",
    "  \n",
    "                loss = self.loss_fn(y_pred, y_truth)\n",
    "                #loss.requires_grad = True\n",
    "        \n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                del batch\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "\n",
    "                mean_loss += float(loss)\n",
    "                batch_n += 1\n",
    "\n",
    "            mean_loss /= batch_n\n",
    "            self.train_loss.append(mean_loss)\n",
    "            print(f'Эпоха: {epoch + 1}\\n Train loss: {mean_loss}\\n {dt.datetime.now() - start} сек.\\n')\n",
    "\n",
    "            NerualNet.eval()\n",
    "            mean_loss = 0\n",
    "            batch_n = 0\n",
    "            with torch.no_grad():\n",
    "                for batch in valid_loader:\n",
    "                    if self.max_batches_per_epoch is not None:\n",
    "                        if batch_n >= self.max_batches_per_epoch:\n",
    "                            break\n",
    "\n",
    "                target = batch[\"label\"].float().to(device)\n",
    "                text_input_ids = batch[\"text_input_ids\"].to(device)\n",
    "                text_input_mask = batch[\"text_input_mask\"].to(device)\n",
    "                text_segment_ids = batch[\"text_segment_ids\"].to(device)\n",
    "                text_offset_mapping = batch[\"text_offset_mapping\"].to(device)\n",
    "                text_pos = batch[\"text_pos\"].to(device)\n",
    "                def_input_ids = batch[\"def_input_ids\"].to(device)\n",
    "                def_input_mask = batch[\"def_input_mask\"].to(device)\n",
    "                def_segment_ids = batch[\"def_segment_ids\"].to(device)\n",
    "\n",
    "                predicted_values = NerualNet(text_input_ids, text_input_mask, text_segment_ids, text_offset_mapping,\n",
    "                                             text_pos, def_input_ids, def_input_mask, def_segment_ids).float()\n",
    "                \n",
    "                \n",
    "                loss = self.loss_fn(predicted_values, target)\n",
    "\n",
    "                del batch\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "\n",
    "                mean_loss += float(loss)\n",
    "                batch_n += 1\n",
    "\n",
    "            mean_loss /= batch_n\n",
    "            self.valid_loss.append(mean_loss)\n",
    "            print(f'Loss_val: {mean_loss}')\n",
    "\n",
    "            if mean_loss < best_val_loss:\n",
    "                self.best_model = NerualNet\n",
    "                best_val_loss = mean_loss\n",
    "                best_ep = epoch\n",
    "            elif epoch - best_ep > self.early_stopping:\n",
    "                print(f'{self.early_stopping} без улучшений. Прекращаем обучение...')\n",
    "                break\n",
    "            if self.scheduler is not None:\n",
    "                scheduler.step()\n",
    "            print()\n",
    "\n",
    "        print(\"-=-=-=-=-=-=-=-=-=-= Evaluation of the best model =-=-=-=-=-=-=-=-=-=-\")\n",
    "        plt.plot(range(len(self.train_loss)), self.train_loss, color='green', label='train', linestyle='solid')\n",
    "        plt.plot(range(len(self.valid_loss)), self.valid_loss, color='red', label='val', linestyle='solid')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_test = [float(sample['label']) for sample in valid_dataset]\n",
    "            Y_pred = []\n",
    "            Y_pred = [self.best_model(sample['text_input_ids'].unsqueeze(0).to(device), \n",
    "                                      sample['text_input_mask'].unsqueeze(0).to(device),\n",
    "                                      sample['text_segment_ids'].unsqueeze(0).to(device),\n",
    "                                      sample['text_offset_mapping'].unsqueeze(0).to(device),\n",
    "                                      sample['text_pos'].unsqueeze(0).to(device),\n",
    "                                      sample['def_input_ids'].unsqueeze(0).to(device),\n",
    "                                      sample['def_input_mask'].unsqueeze(0).to(device),\n",
    "                                      sample['def_segment_ids'].unsqueeze(0).to(device)) for sample in valid_dataset]\n",
    "            Y_pred = [float(y > 0.5) for y in Y_pred]\n",
    "            print()\n",
    "\n",
    "            print(f\"report: \\n\", classification_report(y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6dd98c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preparation(texts, definitions, position, labels, tokenizer, max_len):\n",
    "    tokenizer = tokenizer\n",
    "    feautures_X, feautures_Y = [], []\n",
    "\n",
    "    for i, (text, definition) in enumerate(zip(texts, definitions)):\n",
    "        text = tokenizer(text, return_offsets_mapping=True,max_length=max_len,truncation=True,padding='max_length')\n",
    "\n",
    "        text_input_ids = text[\"input_ids\"]\n",
    "        text_input_mask = text[\"attention_mask\"]\n",
    "        text_segment_ids = text[\"token_type_ids\"]\n",
    "        text_offset_mapping = text[\"offset_mapping\"]\n",
    "        text_pos = [position[i]]\n",
    "\n",
    "        definition = tokenizer(definition, return_offsets_mapping=True,max_length=max_len,padding='max_length',truncation=True)\n",
    "\n",
    "        def_input_ids = definition[\"input_ids\"]\n",
    "        def_input_mask = definition[\"attention_mask\"]\n",
    "        def_segment_ids = definition[\"token_type_ids\"]\n",
    "\n",
    "        feautures_X.append([text_input_ids, text_input_mask, text_segment_ids, text_offset_mapping,\n",
    "                            text_pos, def_input_ids, def_input_mask, def_segment_ids])\n",
    "        feautures_Y.append(labels[i])\n",
    "\n",
    "    return feautures_X, feautures_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a2096cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65dd3b37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv('../../nn_data.csv')\n",
    "df.position = df.position.apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "max_len_text = df.text.str.len().max()\n",
    "max_len_def = df.definition.str.len().max()\n",
    "\n",
    "max_len = max_len_def\n",
    "if max_len_text > max_len_def:\n",
    "    max_len = max_len_text\n",
    "\n",
    "data_X, data_Y = data_preparation(df.text,\n",
    "                                  df.definition,\n",
    "                                  df.position,\n",
    "                                  df.label,\n",
    "                                  BertTokenizerFast.from_pretrained('sberbank-ai/sbert_large_mt_nlu_ru',\n",
    "                                                                do_lower_case=True),\n",
    "                                  max_len)\n",
    "\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(data_X, data_Y, test_size = 0.2, random_state=42)\n",
    "\n",
    "train_dataset = DisambiguationDataset(train_X, train_Y)\n",
    "test_dataset = DisambiguationDataset(test_X, test_Y)\n",
    "\n",
    "# trainer = Trainer(num_epochs=40,\n",
    "#                   batch_size=4,\n",
    "#                   loss_fn=nn.BCELoss(),\n",
    "#                   model=NerualNet(max_seq_len=max_len, device='cuda:0'),\n",
    "#                   device='cuda:0')\n",
    "\n",
    "# trainer.fit(train_dataset=train_dataset, valid_dataset=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa16541e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save_model(\"bert_fc_dropout_1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c57b7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#                 target = batch[\"label\"].float().to(device)\n",
    "#                 text_input_ids = batch[\"text_input_ids\"].to(device)\n",
    "#                 text_input_mask = batch[\"text_input_mask\"].to(device)\n",
    "#                 text_segment_ids = batch[\"text_segment_ids\"].to(device)\n",
    "#                 text_offset_mapping = batch[\"text_offset_mapping\"].to(device)\n",
    "#                 text_pos = batch[\"text_pos\"].to(device)\n",
    "#                 def_input_ids = batch[\"def_input_ids\"].to(device)\n",
    "#                 def_input_mask = batch[\"def_input_mask\"].to(device)\n",
    "#                 def_segment_ids = batch[\"def_segment_ids\"].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "490709f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "4096\n",
      "8192\n",
      "492838912\n",
      "494936064\n",
      "494944256\n",
      "494948352\n",
      "494952448\n",
      "499146752\n",
      "499150848\n",
      "503345152\n",
      "503349248\n",
      "507543552\n",
      "507547648\n",
      "511741952\n",
      "511746048\n",
      "511750144\n",
      "511754240\n",
      "528531456\n",
      "528547840\n",
      "545325056\n",
      "545329152\n",
      "545333248\n",
      "545337344\n",
      "549531648\n",
      "549535744\n",
      "553730048\n",
      "553734144\n",
      "557928448\n",
      "557932544\n",
      "562126848\n",
      "562130944\n",
      "562135040\n",
      "562139136\n",
      "578916352\n",
      "578932736\n",
      "595709952\n",
      "595714048\n",
      "595718144\n",
      "595722240\n",
      "599916544\n",
      "599920640\n",
      "604114944\n",
      "604119040\n",
      "608313344\n",
      "608317440\n",
      "612511744\n",
      "612515840\n",
      "612519936\n",
      "612524032\n",
      "629301248\n",
      "629317632\n",
      "646094848\n",
      "646098944\n",
      "646103040\n",
      "646107136\n",
      "650301440\n",
      "650305536\n",
      "654499840\n",
      "654503936\n",
      "658698240\n",
      "658702336\n",
      "662896640\n",
      "662900736\n",
      "662904832\n",
      "662908928\n",
      "679686144\n",
      "679702528\n",
      "696479744\n",
      "696483840\n",
      "696487936\n",
      "696492032\n",
      "700686336\n",
      "700690432\n",
      "704884736\n",
      "704888832\n",
      "709083136\n",
      "709087232\n",
      "713281536\n",
      "713285632\n",
      "713289728\n",
      "713293824\n",
      "730071040\n",
      "730087424\n",
      "746864640\n",
      "746868736\n",
      "746872832\n",
      "746876928\n",
      "751071232\n",
      "751075328\n",
      "755269632\n",
      "755273728\n",
      "759468032\n",
      "759472128\n",
      "763666432\n",
      "763670528\n",
      "763674624\n",
      "763678720\n",
      "780455936\n",
      "780472320\n",
      "797249536\n",
      "797253632\n",
      "797257728\n",
      "797261824\n",
      "801456128\n",
      "801460224\n",
      "805654528\n",
      "805658624\n",
      "809852928\n",
      "809857024\n",
      "814051328\n",
      "814055424\n",
      "814059520\n",
      "814063616\n",
      "830840832\n",
      "830857216\n",
      "847634432\n",
      "847638528\n",
      "847642624\n",
      "847646720\n",
      "851841024\n",
      "851845120\n",
      "856039424\n",
      "856043520\n",
      "860237824\n",
      "860241920\n",
      "864436224\n",
      "864440320\n",
      "864444416\n",
      "864448512\n",
      "881225728\n",
      "881242112\n",
      "898019328\n",
      "898023424\n",
      "898027520\n",
      "898031616\n",
      "902225920\n",
      "902230016\n",
      "906424320\n",
      "906428416\n",
      "910622720\n",
      "910626816\n",
      "914821120\n",
      "914825216\n",
      "914829312\n",
      "914833408\n",
      "931610624\n",
      "931627008\n",
      "948404224\n",
      "948408320\n",
      "948412416\n",
      "948416512\n",
      "952610816\n",
      "952614912\n",
      "956809216\n",
      "956813312\n",
      "961007616\n",
      "961011712\n",
      "965206016\n",
      "965210112\n",
      "965214208\n",
      "965218304\n",
      "981995520\n",
      "982011904\n",
      "998789120\n",
      "998793216\n",
      "998797312\n",
      "998801408\n",
      "1002995712\n",
      "1002999808\n",
      "1007194112\n",
      "1007198208\n",
      "1011392512\n",
      "1011396608\n",
      "1015590912\n",
      "1015595008\n",
      "1015599104\n",
      "1015603200\n",
      "1032380416\n",
      "1032396800\n",
      "1049174016\n",
      "1049178112\n",
      "1049182208\n",
      "1049186304\n",
      "1053380608\n",
      "1053384704\n",
      "1057579008\n",
      "1057583104\n",
      "1061777408\n",
      "1061781504\n",
      "1065975808\n",
      "1065979904\n",
      "1065984000\n",
      "1065988096\n",
      "1082765312\n",
      "1082781696\n",
      "1099558912\n",
      "1099563008\n",
      "1099567104\n",
      "1099571200\n",
      "1103765504\n",
      "1103769600\n",
      "1107963904\n",
      "1107968000\n",
      "1112162304\n",
      "1112166400\n",
      "1116360704\n",
      "1116364800\n",
      "1116368896\n",
      "1116372992\n",
      "1133150208\n",
      "1133166592\n",
      "1149943808\n",
      "1149947904\n",
      "1149952000\n",
      "1149956096\n",
      "1154150400\n",
      "1154154496\n",
      "1158348800\n",
      "1158352896\n",
      "1162547200\n",
      "1162551296\n",
      "1166745600\n",
      "1166749696\n",
      "1166753792\n",
      "1166757888\n",
      "1183535104\n",
      "1183551488\n",
      "1200328704\n",
      "1200332800\n",
      "1200336896\n",
      "1200340992\n",
      "1204535296\n",
      "1204539392\n",
      "1208733696\n",
      "1208737792\n",
      "1212932096\n",
      "1212936192\n",
      "1217130496\n",
      "1217134592\n",
      "1217138688\n",
      "1217142784\n",
      "1233920000\n",
      "1233936384\n",
      "1250713600\n",
      "1250717696\n",
      "1250721792\n",
      "1250725888\n",
      "1254920192\n",
      "1254924288\n",
      "1259118592\n",
      "1259122688\n",
      "1263316992\n",
      "1263321088\n",
      "1267515392\n",
      "1267519488\n",
      "1267523584\n",
      "1267527680\n",
      "1284304896\n",
      "1284321280\n",
      "1301098496\n",
      "1301102592\n",
      "1301106688\n",
      "1301110784\n",
      "1305305088\n",
      "1305309184\n",
      "1309503488\n",
      "1309507584\n",
      "1313701888\n",
      "1313705984\n",
      "1317900288\n",
      "1317904384\n",
      "1317908480\n",
      "1317912576\n",
      "1334689792\n",
      "1334706176\n",
      "1351483392\n",
      "1351487488\n",
      "1351491584\n",
      "1351495680\n",
      "1355689984\n",
      "1355694080\n",
      "1359888384\n",
      "1359892480\n",
      "1364086784\n",
      "1364090880\n",
      "1368285184\n",
      "1368289280\n",
      "1368293376\n",
      "1368297472\n",
      "1385074688\n",
      "1385091072\n",
      "1401868288\n",
      "1401872384\n",
      "1401876480\n",
      "1401880576\n",
      "1406074880\n",
      "1406078976\n",
      "1410273280\n",
      "1410277376\n",
      "1414471680\n",
      "1414475776\n",
      "1418670080\n",
      "1418674176\n",
      "1418678272\n",
      "1418682368\n",
      "1435459584\n",
      "1435475968\n",
      "1452253184\n",
      "1452257280\n",
      "1452261376\n",
      "1452265472\n",
      "1456459776\n",
      "1456463872\n",
      "1460658176\n",
      "1460662272\n",
      "1464856576\n",
      "1464860672\n",
      "1469054976\n",
      "1469059072\n",
      "1469063168\n",
      "1469067264\n",
      "1485844480\n",
      "1485860864\n",
      "1502638080\n",
      "1502642176\n",
      "1502646272\n",
      "1502650368\n",
      "1506844672\n",
      "1506848768\n",
      "1511043072\n",
      "1511047168\n",
      "1515241472\n",
      "1515245568\n",
      "1519439872\n",
      "1519443968\n",
      "1519448064\n",
      "1519452160\n",
      "1536229376\n",
      "1536245760\n",
      "1553022976\n",
      "1553027072\n",
      "1553031168\n",
      "1553035264\n",
      "1557229568\n",
      "1557233664\n",
      "1561427968\n",
      "1561432064\n",
      "1565626368\n",
      "1565630464\n",
      "1569824768\n",
      "1569828864\n",
      "1569832960\n",
      "1569837056\n",
      "1586614272\n",
      "1586630656\n",
      "1603407872\n",
      "1603411968\n",
      "1603416064\n",
      "1603420160\n",
      "1607614464\n",
      "1607618560\n",
      "1611812864\n",
      "1611816960\n",
      "1616011264\n",
      "1616015360\n",
      "1620209664\n",
      "1620213760\n",
      "1620217856\n",
      "1620221952\n",
      "1636999168\n",
      "1637015552\n",
      "1653792768\n",
      "1653796864\n",
      "1653800960\n",
      "1653805056\n",
      "1657999360\n",
      "1658003456\n",
      "1662197760\n",
      "1662201856\n",
      "1666396160\n",
      "1666400256\n",
      "1670594560\n",
      "1670598656\n",
      "1670602752\n",
      "1670606848\n",
      "1687384064\n",
      "1687400448\n",
      "1704177664\n",
      "1704181760\n",
      "1704185856\n",
      "1704189952\n",
      "1708384256\n",
      "1708388352\n",
      "1710485504\n",
      "1710487040\n",
      "1712010752\n",
      "1712012288\n",
      "1712013824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\y3g9r\\AppData\\Local\\Temp\\ipykernel_36196\\761441240.py:131: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embeddings_data = torch.tensor(\n",
      "C:\\Users\\y3g9r\\AppData\\Local\\Temp\\ipykernel_36196\\761441240.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embeddings_data = torch.tensor(tokens_embeddings_ex[tokens_key_word_position_ex[0]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.69      0.63       106\n",
      "         1.0       0.55      0.45      0.49        92\n",
      "\n",
      "    accuracy                           0.58       198\n",
      "   macro avg       0.57      0.57      0.56       198\n",
      "weighted avg       0.57      0.58      0.57       198\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "device='cuda:0'\n",
    "with torch.no_grad():\n",
    "            model = NerualNet(max_seq_len=max_len, device='cuda:0')\n",
    "            model = torch.load(\"bert_fc_dropout_1.pth\")\n",
    "            model.to(device)\n",
    "            y_test = [float(sample['label']) for sample in test_dataset]\n",
    "            Y_pred = []\n",
    "            Y_pred = [model(sample['text_input_ids'].unsqueeze(0).to(device), \n",
    "                                      sample['text_input_mask'].unsqueeze(0).to(device),\n",
    "                                      sample['text_segment_ids'].unsqueeze(0).to(device),\n",
    "                                      sample['text_offset_mapping'].unsqueeze(0).to(device),\n",
    "                                      sample['text_pos'].unsqueeze(0).to(device),\n",
    "                                      sample['def_input_ids'].unsqueeze(0).to(device),\n",
    "                                      sample['def_input_mask'].unsqueeze(0).to(device),\n",
    "                                      sample['def_segment_ids'].unsqueeze(0).to(device)) for sample in test_dataset]\n",
    "            Y_pred = [float(y > 0.5) for y in Y_pred]\n",
    "            print()\n",
    "\n",
    "            print(f\"report: \\n\",classification_report(y_test,Y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
