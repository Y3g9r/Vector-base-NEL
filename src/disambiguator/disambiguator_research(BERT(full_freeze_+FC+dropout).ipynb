{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23862,"status":"ok","timestamp":1677077129545,"user":{"displayName":"y3g0r k","userId":"03339002915347279062"},"user_tz":-180},"id":"DI09IyB3lY7h","outputId":"78657107-0cde-49b2-cca6-9644fffdf4be"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting huggingface-hub\u003c1.0,\u003e=0.11.0\n","  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n","Collecting tokenizers!=0.11.3,\u003c0.14,\u003e=0.11.1\n","  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyyaml\u003e=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: tqdm\u003e=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: typing-extensions\u003e=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.11.0-\u003etransformers) (4.5.0)\n","Requirement already satisfied: chardet\u003c5,\u003e=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests-\u003etransformers) (4.0.0)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests-\u003etransformers) (2022.12.7)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.8/dist-packages (from requests-\u003etransformers) (2.10)\n","Requirement already satisfied: urllib3\u003c1.27,\u003e=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests-\u003etransformers) (1.24.3)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.12.1 tokenizers-0.13.2 transformers-4.26.1\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":398,"status":"ok","timestamp":1677077162013,"user":{"displayName":"y3g0r k","userId":"03339002915347279062"},"user_tz":-180},"id":"10309577","scrolled":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import  Dataset\n","from torch.utils.data import DataLoader\n","from transformers import BertTokenizerFast, BertModel\n","import torch.optim as optim\n","\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","import matplotlib.pyplot as plt\n","\n","import ast\n","import datetime as dt\n","import gc"]},{"cell_type":"markdown","metadata":{"id":"760be49e"},"source":["# Обучение модели"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1677077163348,"user":{"displayName":"y3g0r k","userId":"03339002915347279062"},"user_tz":-180},"id":"7bb0542d"},"outputs":[],"source":["class DisambiguationDataset(Dataset):\n","    def __init__(self, samples,labels):\n","        self.samples = samples\n","        self.labels = labels\n","        self.len = len(self.samples)\n","\n","    def __len__(self):\n","        return self.len\n","\n","    def __getitem__(self, index):\n","        items = {\"text_input_ids\": torch.tensor(self.samples[index][0]),\n","                 \"text_input_mask\": torch.tensor(self.samples[index][1]),\n","                 \"text_segment_ids\": torch.tensor(self.samples[index][2]),\n","                 \"text_offset_mapping\": torch.tensor(self.samples[index][3]),\n","                 \"text_pos\": torch.tensor(self.samples[index][4]),\n","                 \"def_input_ids\": torch.tensor(self.samples[index][5]),\n","                 \"def_input_mask\": torch.tensor(self.samples[index][6]),\n","                 \"def_segment_ids\": torch.tensor(self.samples[index][7]),\n","                 \"label\": torch.tensor(self.labels[index])}\n","        return items"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1677077165100,"user":{"displayName":"y3g0r k","userId":"03339002915347279062"},"user_tz":-180},"id":"269a9166"},"outputs":[],"source":["class NerualNet(nn.Module):\n","    def __init__(self, hidden_size=768, max_seq_len=388, device='cpu'):\n","        self.device = device\n","        super(NerualNet, self).__init__()\n","        self.bert = BertModel.from_pretrained('sberbank-ai/sbert_large_mt_nlu_ru', output_hidden_states=True,\n","                                              return_dict=False)\n","        for layer in self.bert.encoder.layer:\n","            for param in layer.parameters():\n","                param.requires_grad = False\n","\n","        self.text_linear_1 = torch.nn.Linear(1024, max_seq_len)\n","        self.def_linear_1 = torch.nn.Linear(1024, max_seq_len)\n","        \n","        self.Dropout_text = torch.nn.Dropout(0.5)\n","        self.Dropout_def = torch.nn.Dropout(0.5)\n","        \n","        self.sigm_linear_1 = torch.nn.Linear(max_seq_len, 1)\n","        self.Dropout_sigm = torch.nn.Dropout(0.5)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, text_input_ids, text_input_mask, text_segment_ids, text_offset_mapping,\n","                text_pos, def_input_ids, def_input_mask, def_segment_ids):\n","\n","        embd_batch = torch.tensor([[[], []]]).to(self.device)\n","        first_pass = False\n","        for i in range(len(text_input_ids)):\n","            # получаем эмбединги ключевого слова из примера употребления\n","            examples_token_key_word_position = self.token_detection(text_offset_mapping[i], text_pos[i][0])\n","            example_token_vec = self.get_vector(text_input_ids[i], text_segment_ids[i], text_input_mask[i])\n","            example_embeddings = self.vector_recognition(example_token_vec, examples_token_key_word_position)\n","\n","            # получаем эмбединг определения\n","            def_embedding = self.get_defenition_embedding(def_input_ids[i], def_segment_ids[i],\n","                                                          def_input_mask[i]).squeeze(0)\n","            # объединяем два вектора в 1 и добавляем в общий массив (получаем тензор 2x768)\n","            embd_sample = torch.stack((example_embeddings, def_embedding)).to(self.device)\n","            if not first_pass:\n","                embd_batch = torch.cat((embd_batch, embd_sample.unsqueeze(0)), -1)\n","                first_pass = True\n","            else:\n","                embd_batch = torch.cat((embd_batch, embd_sample.unsqueeze(0)), 0)\n","\n","                \n","        text_emb = embd_batch[:, 0, :]\n","        def_emb = embd_batch[:, 1, :]\n","\n","        ex_emb = self.Dropout_text(self.text_linear_1(text_emb))\n","        def_emb = self.Dropout_def(self.def_linear_1(def_emb))\n","\n","        dist = torch.abs(ex_emb-def_emb)\n","        py = self.Dropout_sigm(self.sigm_linear_1(dist))\n","\n","        y = self.sigmoid(py).permute(1,0).squeeze(0)\n","\n","        return y\n","\n","    def get_defenition_embedding(self, def_input_ids, def_segment_ids, def_input_mask):\n","        \"\"\"\n","        Функция получения вектора дефенишина сущности\n","        :param def_input_ids:\n","        :param def_segment_ids:\n","        :param def_input_mask:\n","        :return: bert pooler output vector\n","        \"\"\"\n","        with torch.no_grad():\n","            output = self.bert(input_ids=def_input_ids.unsqueeze(0), token_type_ids=def_segment_ids.unsqueeze(0),\n","                               attention_mask=def_input_mask.unsqueeze(0))\n","        hidden_states = output[1]\n","        return hidden_states\n","\n","    def token_detection(self, token_map, position):\n","        \"\"\"\n","        Функция определения ключевого слова\n","        :param token_map: list of tuples of begin and end of every token\n","        :param position:  list of type: [int,int]\n","        :return: list of key word tokens position\n","        \"\"\"\n","        # из за того что в начале стоит CLS позиции начала и конца ключевого слова сдвигаются на 5\n","        begin_postion = position[0]  # + 5\n","        end_position = position[1]  # + 5\n","\n","        position_of_key_tokens = []\n","        for token_tuple in range(1, len(token_map) - 1):\n","            # Если ключевое слово представляется одним токеном\n","            if token_map[token_tuple][0] == begin_postion and token_map[token_tuple][1] == end_position:\n","                position_of_key_tokens.append(token_tuple)\n","                break\n","\n","            # Если ключевое слово представляется несколькими токенами\n","            if token_map[token_tuple][0] \u003e= begin_postion and token_map[token_tuple][1] != end_position:\n","                position_of_key_tokens.append(token_tuple)\n","            if token_map[token_tuple][0] != begin_postion and token_map[token_tuple][1] == end_position:\n","                position_of_key_tokens.append(token_tuple)\n","                break\n","\n","        return position_of_key_tokens\n","\n","    def get_vector(self, input_ids_samp, token_type_ids_samp, attention_mask_samp):\n","        \"\"\"\n","        Функция получения вектора ключевого слова\n","        :param input_ids_samp:\n","        :param token_type_ids_samp:\n","        :param attention_mask_samp:\n","        :return:\n","        \"\"\"\n","        with torch.no_grad():\n","            outputs = self.bert(input_ids=input_ids_samp.unsqueeze(0), token_type_ids=token_type_ids_samp.unsqueeze(0),\n","                                attention_mask=attention_mask_samp.unsqueeze(0))\n","        hidden_states = outputs[2]\n","\n","        # из [# layers, # batches, # tokens, # features]\n","        # в [# tokens, # layers, # features]\n","        token_dim = torch.stack(hidden_states, dim=0)\n","        token_dim = torch.squeeze(token_dim, dim=1)\n","        token_dim = token_dim.permute(1, 0, 2)\n","        token_vecs_cat = []\n","        for token in token_dim:\n","            cat_vec = torch.sum(token[-4:], dim=0)\n","            token_vecs_cat.append(cat_vec)\n","        return token_vecs_cat\n","\n","\n","    def vector_recognition(self, tokens_embeddings_ex, tokens_key_word_position_ex):\n","        \"\"\"\n","        Функция подготовки вектора в зависимости от количества токенов,которым представляется ключевое слово\n","        :param tokens_embeddings_ex:\n","        :param tokens_key_word_position_ex:\n","        :return:\n","        \"\"\"\n","        if len(tokens_key_word_position_ex) \u003e 1:\n","            embeddings_data = torch.tensor(\n","                self.__get_avarage_embedding(tokens_embeddings_ex, tokens_key_word_position_ex))\n","        else:\n","            embeddings_data = torch.tensor(tokens_embeddings_ex[tokens_key_word_position_ex[0]])\n","        return embeddings_data\n","\n","    def __get_avarage_embedding(self, embeddings_list, positions_list):\n","        \"\"\"\n","        Функция получения среднего вектора (применяется в случае если ключевое слово состоит из нескольких токенов)\n","        :param embeddings_list:\n","        :param positions_list:\n","        :return:\n","        \"\"\"\n","        avg_tensor = torch.stack((embeddings_list[positions_list[0]],))\n","        for i in range(1, len(positions_list)):\n","            avg_tensor = torch.cat((avg_tensor, embeddings_list[positions_list[i]].unsqueeze(0)))\n","\n","        average_embedding = torch.mean(avg_tensor, 0)\n","        return average_embedding"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":247,"status":"ok","timestamp":1677077303727,"user":{"displayName":"y3g0r k","userId":"03339002915347279062"},"user_tz":-180},"id":"c66ce6fd"},"outputs":[],"source":["class Trainer():\n","    def __init__(self, num_epochs=None, batch_size=None,\n","                 max_batches_per_epoch=None, early_stopping=10,\n","                 loss_fn=None, optimizer=None, model=None,\n","                 scheduler=None, device='cpu'):\n","        self.num_epochs = num_epochs\n","        self.batch_size = batch_size\n","        self.max_batches_per_epoch = max_batches_per_epoch\n","        self.early_stopping = early_stopping\n","        self.loss_fn = loss_fn\n","        self.device = device\n","        self.optimizer = optimizer\n","        self.scheduler = scheduler\n","        self.start_model = model\n","        self.best_model = model\n","\n","        self.train_loss = []\n","        self.valid_loss = []\n","\n","    def predict(self, input_ids, input_mask, segment_ids):\n","        return self.best_model(input_ids, input_mask, segment_ids)\n","\n","    def save_model(self, path: str):\n","        try:\n","            torch.save(self.best_model, path)\n","        except Exception as e:\n","            print(f\"Не удалось сохранить модель. Ошибка {e}\")\n","            exit(1)\n","\n","        return True\n","\n","    def load_model(self, path: str):\n","        try:\n","            self.best_model.load_state_dict(torch.load(path))\n","        except Exception as e:\n","            print(f\"Не удалось загрузить модель. Ошибка {e}\")\n","            exit(1)\n","\n","        return True\n","\n","    def fit(self, train_dataset, valid_dataset):\n","        device = torch.device(self.device)\n","        NerualNet = self.start_model\n","        NerualNet.to(device)\n","\n","        NerualNet.train()\n","\n","        self.optimizer = optim.Adam(NerualNet.parameters(), lr=0.0001)\n","\n","        train_loader = DataLoader(dataset=train_dataset, batch_size=self.batch_size,\n","                                  shuffle=False, drop_last=True)\n","        valid_loader = DataLoader(dataset=valid_dataset, batch_size=self.batch_size,\n","                                  shuffle=False, drop_last=True)\n","\n","        best_val_loss = float('inf')  # Лучшее значение функции потерь на валидационной выборке\n","\n","        best_ep = 0  # Эпоха, на которой достигалось лучшее значение функции потерь на валидационной выборке\n","\n","        for epoch in range(self.num_epochs):\n","            start = dt.datetime.now()\n","            mean_loss = 0\n","            batch_n = 0\n","            for batch in train_loader:\n","                y_truth = batch[\"label\"].float().to(device)\n","                text_input_ids = batch[\"text_input_ids\"].to(device)\n","                text_input_mask = batch[\"text_input_mask\"].to(device)\n","                text_segment_ids = batch[\"text_segment_ids\"].to(device)\n","                text_offset_mapping = batch[\"text_offset_mapping\"].to(device)\n","                text_pos = batch[\"text_pos\"].to(device)\n","                def_input_ids = batch[\"def_input_ids\"].to(device)\n","                def_input_mask = batch[\"def_input_mask\"].to(device)\n","                def_segment_ids = batch[\"def_segment_ids\"].to(device)\n","                y_pred = NerualNet(text_input_ids, text_input_mask, text_segment_ids, text_offset_mapping,\n","                                   text_pos, def_input_ids, def_input_mask, def_segment_ids).float()\n","  \n","                loss = self.loss_fn(y_pred, y_truth)\n","                #loss.requires_grad = True\n","        \n","                self.optimizer.zero_grad()\n","                loss.backward()\n","                self.optimizer.step()\n","\n","                del batch\n","                torch.cuda.empty_cache()\n","                gc.collect()\n","\n","                mean_loss += float(loss)\n","                batch_n += 1\n","\n","            mean_loss /= batch_n\n","            self.train_loss.append(mean_loss)\n","            print(f'Эпоха: {epoch + 1}\\n Train loss: {mean_loss}\\n {dt.datetime.now() - start} сек.\\n')\n","\n","            NerualNet.eval()\n","            mean_loss = 0\n","            batch_n = 0\n","            with torch.no_grad():\n","                for batch in valid_loader:\n","                    if self.max_batches_per_epoch is not None:\n","                        if batch_n \u003e= self.max_batches_per_epoch:\n","                            break\n","\n","                target = batch[\"label\"].float().to(device)\n","                text_input_ids = batch[\"text_input_ids\"].to(device)\n","                text_input_mask = batch[\"text_input_mask\"].to(device)\n","                text_segment_ids = batch[\"text_segment_ids\"].to(device)\n","                text_offset_mapping = batch[\"text_offset_mapping\"].to(device)\n","                text_pos = batch[\"text_pos\"].to(device)\n","                def_input_ids = batch[\"def_input_ids\"].to(device)\n","                def_input_mask = batch[\"def_input_mask\"].to(device)\n","                def_segment_ids = batch[\"def_segment_ids\"].to(device)\n","\n","                predicted_values = NerualNet(text_input_ids, text_input_mask, text_segment_ids, text_offset_mapping,\n","                                             text_pos, def_input_ids, def_input_mask, def_segment_ids).float()\n","                \n","                \n","                loss = self.loss_fn(predicted_values, target)\n","\n","                del batch\n","                torch.cuda.empty_cache()\n","                gc.collect()\n","\n","                mean_loss += float(loss)\n","                batch_n += 1\n","\n","            mean_loss /= batch_n\n","            self.valid_loss.append(mean_loss)\n","            print(f'Loss_val: {mean_loss}')\n","\n","            if mean_loss \u003c best_val_loss:\n","                self.best_model = NerualNet\n","                best_val_loss = mean_loss\n","                best_ep = epoch\n","            elif epoch - best_ep \u003e self.early_stopping:\n","                print(f'{self.early_stopping} без улучшений. Прекращаем обучение...')\n","                break\n","            if self.scheduler is not None:\n","                scheduler.step()\n","            print()\n","\n","        print(\"-=-=-=-=-=-=-=-=-=-= Evaluation of the best model =-=-=-=-=-=-=-=-=-=-\")\n","        plt.plot(range(len(self.train_loss)), self.train_loss, color='green', label='train', linestyle='solid')\n","        plt.plot(range(len(self.valid_loss)), self.valid_loss, color='red', label='val', linestyle='solid')\n","        plt.legend()\n","        plt.show()\n","\n","        with torch.no_grad():\n","            y_test = [float(sample['label']) for sample in valid_dataset]\n","            Y_pred = []\n","            Y_pred = [self.best_model(sample['text_input_ids'].unsqueeze(0).to(device), \n","                                      sample['text_input_mask'].unsqueeze(0).to(device),\n","                                      sample['text_segment_ids'].unsqueeze(0).to(device),\n","                                      sample['text_offset_mapping'].unsqueeze(0).to(device),\n","                                      sample['text_pos'].unsqueeze(0).to(device),\n","                                      sample['def_input_ids'].unsqueeze(0).to(device),\n","                                      sample['def_input_mask'].unsqueeze(0).to(device),\n","                                      sample['def_segment_ids'].unsqueeze(0).to(device)) for sample in valid_dataset]\n","            Y_pred = [float(y \u003e 0.5) for y in Y_pred]\n","            print()\n","\n","            print(f\"report: \\n\", classification_report(y_test, Y_pred))"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1677077305516,"user":{"displayName":"y3g0r k","userId":"03339002915347279062"},"user_tz":-180},"id":"c6dd98c1"},"outputs":[],"source":["def data_preparation(texts, definitions, position, labels, tokenizer, max_len):\n","    tokenizer = tokenizer\n","    feautures_X, feautures_Y = [], []\n","\n","    for i, (text, definition) in enumerate(zip(texts, definitions)):\n","        text = tokenizer(text, return_offsets_mapping=True,max_length=max_len,truncation=True,padding='max_length')\n","\n","        text_input_ids = text[\"input_ids\"]\n","        text_input_mask = text[\"attention_mask\"]\n","        text_segment_ids = text[\"token_type_ids\"]\n","        text_offset_mapping = text[\"offset_mapping\"]\n","        text_pos = [position[i]]\n","\n","        definition = tokenizer(definition, return_offsets_mapping=True,max_length=max_len,padding='max_length',truncation=True)\n","\n","        def_input_ids = definition[\"input_ids\"]\n","        def_input_mask = definition[\"attention_mask\"]\n","        def_segment_ids = definition[\"token_type_ids\"]\n","\n","        feautures_X.append([text_input_ids, text_input_mask, text_segment_ids, text_offset_mapping,\n","                            text_pos, def_input_ids, def_input_mask, def_segment_ids])\n","        feautures_Y.append(labels[i])\n","\n","    return feautures_X, feautures_Y"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":893,"status":"ok","timestamp":1677077307616,"user":{"displayName":"y3g0r k","userId":"03339002915347279062"},"user_tz":-180},"id":"4a2096cb","outputId":"169ca67c-f0ae-4541-bd0a-49e38aa5a4ac"},"outputs":[{"data":{"text/plain":["37"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["torch.cuda.empty_cache()\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"65dd3b37","scrolled":true},"outputs":[{"ename":"RuntimeError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-15-9e53b9752e32\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m                   device='cuda:0')\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 30\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m\u003cipython-input-11-63e1119c975c\u003e\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_dataset, valid_dataset)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mNerualNet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 44\u001b[0;31m         \u001b[0mNerualNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mNerualNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 989\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m     def register_backward_hook(\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 641\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 641\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 641\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    662\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 664\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    665\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    985\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    986\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--\u003e 987\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'CUDA_MODULE_LOADING'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CUDA_MODULE_LOADING'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'LAZY'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 229\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"]}],"source":["\n","\n","\n","df = pd.read_csv('nn_data.csv')\n","df.position = df.position.apply(lambda x: ast.literal_eval(x))\n","\n","max_len_text = df.text.str.len().max()\n","max_len_def = df.definition.str.len().max()\n","\n","max_len = max_len_def\n","if max_len_text \u003e max_len_def:\n","    max_len = max_len_text\n","\n","data_X, data_Y = data_preparation(df.text,\n","                                  df.definition,\n","                                  df.position,\n","                                  df.label,\n","                                  BertTokenizerFast.from_pretrained('sberbank-ai/sbert_large_mt_nlu_ru',\n","                                                                do_lower_case=True),\n","                                  max_len)\n","\n","train_X, test_X, train_Y, test_Y = train_test_split(data_X, data_Y, test_size = 0.2, random_state=42)\n","\n","train_dataset = DisambiguationDataset(train_X, train_Y)\n","test_dataset = DisambiguationDataset(test_X, test_Y)\n","\n","trainer = Trainer(num_epochs=40,\n","                  batch_size=4,\n","                  loss_fn=nn.BCELoss(),\n","                  model=NerualNet(max_seq_len=max_len, device='cuda:0'),\n","                  device='cuda:0')\n","\n","trainer.fit(train_dataset=train_dataset, valid_dataset=test_dataset)"]}],"metadata":{"accelerator":"GPU","colab":{"name":"","version":""},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"}},"nbformat":4,"nbformat_minor":5}